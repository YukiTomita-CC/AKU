- 「データセットにEOSトークンを含めることは将来的に非推奨となる」というワーニングが出たが、今回に関してはEOSトークンを学習できているようだ
- T5Tokenizerがinputをencodeしたときに最後にEOSトークンを出力してしまう...。なぜ？
  - encode時のオプションの問題だった。`add_special_tokens=False`にするとEOSトークンが出力されない
- wikipedia、aozorabunkoのデータセットもEOSトークンを付与しておけばよかった。
  - いや、データセットからランダムにサンプリングされるなら文の順序がおかしくなる
  - 前の文章を参照していることもあるし、今回のように単純に1文ずつのデータセットではそもそも無理か...。
- float32にした理由はbfloat16がまだ多くのCPUでサポートされてないから(AVX512から)
  - 多くのデバイスでほぼフルスペックで動いてほしい+学習を安定させるため
  - でもよく考えたらbfloat16で学習して推論時にfloat16にキャストするでもよかったのかも？
- このモデルなら単精度(float32)でも4GBのVRAMで十分。
- huggingfaceのdatasetは一度使ったデータをcacheする、が、保存先は~/.cacheになるので、RunPodでContainerDiskを使わないからいいやと思って20GBとかにした場合はちゃんとHOMEを/workspaceにしておくこと
- 今回は知識不足もあり、GPU並列処理がうまくいかなかった(4枚使っても速度が25%しか向上しない)
  - NVLinkじゃなかったから？
  - accelerateを使っていないから？
  - 4枚借りてしまっていたため、結局1プロセス1枚で実行したが、かなり無駄な使い方をしてしまった
- バッチサイズを2倍にして学習率も2倍にすれば基本的には最終的なlossは変わらない
- 今回のケースについてはT5TokenizerよりもLlamaTokenizerの方がよかった
  - 改行などの特殊文字をきちんと表示できる
  - しかし、T5TokenizerとBOSなどの特殊トークンの設定方法が異なるようで、語彙数32kをターゲットにしたのにBOSが最後に来たりして語彙数が若干変わってしまった
  - おそらくこれによりGGUFへの変換が上手くいっていない
- 学習率を1.0e-4から8.0e-4に上げたらlossがかなり下がった
  - 今回のモデルは学習が安定しているので、学習率をもっと上げてもよかったかもしれない
