- 「データセットにEOSトークンを含めることは将来的に非推奨となる」というワーニングが出たが、今回に関してはEOSトークンを学習できているようだ
- T5Tokenizerがinputをencodeしたときに最後にEOSトークンを出力してしまう...。なぜ？
- wikipedia、aozorabunkoのデータセットもEOSトークンを付与しておけばよかった。
  - いや、データセットからランダムにサンプリングされるなら文の順序がおかしくなる
  - 前の文章を参照していることもあるし、今回のように単純に1文ずつのデータセットではそもそも無理か...。
- float32にした理由はbfloat16がまだ多くのCPUでサポートされてないから(AVX512から)
  - 多くのデバイスでほぼフルスペックで動いてほしい+学習を安定させるため
  - でもよく考えたらbfloat16で学習して推論時にfloat16にキャストするでもよかったのかも？
- このモデルなら単精度(float32)でも4GBのVRAMで十分。
- huggingfaceのdatasetは一度使ったデータをcacheする、が、保存先は~/.cacheになるので、RunPodでContainerDiskを使わないからいいやと思って20GBとかにした場合はちゃんとHOMEを/workspaceにしておくこと
- 今回は知識不足もあり、GPU並列処理がうまくいかなかった(4枚使っても速度が25%しか向上しない)
  - NVLinkじゃなかったから？
  - accelerateを使っていないから？
  - 4枚借りてしまっていたため、結局1プロセス1枚で実行したが、かなり無駄な使い方をしてしまった
