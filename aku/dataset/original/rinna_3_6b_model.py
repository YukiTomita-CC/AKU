import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

model_path = "models/rinna--japanese-gpt-neox-3.6b-instruction-ppo"

tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)
model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True, torch_dtype="auto").to("cuda")

prompt = [
    {
        "speaker": "ã‚·ã‚¹ãƒ†ãƒ ",
        "text": "ã‚„ã£ã»ãƒ¼ã€æœ€è¿‘ä½•ã‹é¢ç™½ã„ã“ã¨ã‚ã£ãŸï¼Ÿ"
    },
    {
        "speaker": "ãƒ¦ãƒ¼ã‚¶ãƒ¼",
        "text": "ãˆãƒ¼ã€é¢ç™½ã„ã“ã¨ï¼ŸğŸ™„æœ€è¿‘æ–°ã—ã„æ¼«ç”»èª­ã¿å§‹ã‚ãŸã“ã¨ãã‚‰ã„ã‹ãªï¼Ÿ"
    }
]
prompt = [
    f"{uttr['speaker']}: {uttr['text']}"
    for uttr in prompt
]
prompt = "<NL>".join(prompt)
prompt = (
    prompt
    + "<NL>"
    + "ã‚·ã‚¹ãƒ†ãƒ : "
)
print(prompt.replace("<NL>", "\n"), end="")

token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors="pt").to("cuda")

with torch.no_grad():
    output_ids = model.generate(
        token_ids,
        do_sample=True,
        max_new_tokens=32,
        temperature=0.95,
        pad_token_id=tokenizer.pad_token_id,
        bos_token_id=tokenizer.bos_token_id,
        eos_token_id=tokenizer.eos_token_id
    )

output = tokenizer.decode(output_ids.tolist()[0][token_ids.size(1):])
# output = output.replace("<NL>", "\n")
print(output)
